{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e9339d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984.txt\n"
     ]
    }
   ],
   "source": [
    "from os.path import basename\n",
    "\n",
    "my_text = 'https://raw.githubusercontent.com/peterverhaar/dmt-2024-assignment3/refs/heads/main/1984.txt'\n",
    "file_name = basename(my_text)\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93899f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install stanza\n",
    "!pip install vaderSentiment\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import requests\n",
    "from os.path import basename\n",
    "\n",
    "def download(url):\n",
    "    response = requests.get(url)\n",
    "    if response:\n",
    "        file_name = basename(url)\n",
    "        out = open(file_name,'w',encoding='utf-8')\n",
    "        out.write(response.text)\n",
    "        out.close()\n",
    "        \n",
    "def sorted_by_value( dict , ascending = True ):\n",
    "    if ascending: \n",
    "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
    "    else:\n",
    "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n",
    "        \n",
    "download('https://raw.githubusercontent.com/peterverhaar/dmt-2024-assignment3/refs/heads/main/Assignment3.ipynb')\n",
    "download('https://raw.githubusercontent.com/peterverhaar/dmt-2024-assignment3/refs/heads/main/text_mining.py')\n",
    "download(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab22f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_mining import *\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize,sent_tokenize,pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.text import Text\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1518b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(file_name,encoding='utf-8')\n",
    "full_text = text_file.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f9409f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel contains 101813 words\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of words\n",
    "words = word_tokenize(full_text.lower())\n",
    "words = remove_punctuation(words)\n",
    "nr_tokens = len(words)\n",
    "\n",
    "print(f'The novel contains {nr_tokens} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9dbf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel contains 8600 unique words\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = Counter(words)\n",
    "print(f'The novel contains {len(word_frequencies)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391f8112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 20 words are most common:\n",
      "the => 6473\n",
      "of => 3485\n",
      "a => 2573\n",
      "and => 2417\n",
      "to => 2333\n",
      "was => 2317\n",
      "he => 1965\n",
      "it => 1862\n",
      "in => 1855\n",
      "that => 1479\n",
      "had => 1346\n",
      "his => 1086\n",
      "you => 915\n",
      "not => 855\n",
      "with => 789\n",
      "as => 721\n",
      "at => 662\n",
      "for => 661\n",
      "be => 657\n",
      "they => 638\n"
     ]
    }
   ],
   "source": [
    "nr_words = 20 \n",
    "\n",
    "print(f'The following {nr_words} words are most common:')\n",
    "\n",
    "for word,count in word_frequencies.most_common(nr_words):\n",
    "    print(f\"{word} => {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac1c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When we remove the stopwords, the following 20 words are most common:\n",
      "winston 522\n",
      "could 431\n",
      "one 426\n",
      "would 362\n",
      "said 341\n",
      "even 306\n",
      "party 289\n",
      "like 206\n",
      "time 199\n",
      "face 194\n",
      "thought 187\n",
      "seemed 173\n",
      "never 173\n",
      "two 163\n",
      "back 153\n",
      "moment 152\n",
      "still 150\n",
      "man 148\n",
      "always 145\n",
      "know 133\n",
      "way 127\n",
      "almost 127\n",
      "another 124\n",
      "words 123\n",
      "though 122\n",
      "might 122\n",
      "war 121\n",
      "eyes 120\n",
      "years 120\n",
      "little 114\n"
     ]
    }
   ],
   "source": [
    "nr_words = 20 \n",
    "\n",
    "words = remove_punctuation_and_stopwords(words)\n",
    "word_frequencies = Counter(words)\n",
    "\n",
    "print(f'When we remove the stopwords, the following {nr_words} words are most common:')\n",
    "\n",
    "for word,count in word_frequencies.most_common(30):\n",
    "    print(f\"{word} {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849f990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel contains 6696 sentences.\n",
      "The sentences contain 15.21 words on average\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(full_text)\n",
    "print(f'The novel contains {len(sentences)} sentences.')\n",
    "avg_nr_words = round((nr_tokens/len(sentences)),2) \n",
    "print(f'The sentences contain {avg_nr_words} words on average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b25de79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no matches\n"
     ]
    }
   ],
   "source": [
    "novel = Text(words)\n",
    "novel.concordance('power' , width = 60 , lines = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8377d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 10\n",
      "curious 1\n",
      "seemed 4\n",
      "merely 2\n",
      "lost 3\n",
      "expressing 1\n",
      "even 7\n",
      "forgotten 1\n",
      "originally 1\n",
      "intended 1\n",
      "say 1\n",
      "the 17\n",
      "face 6\n",
      "screen 2\n",
      "terrifying 1\n",
      "eurasian 1\n",
      "army 1\n",
      "behind 1\n",
      "much 2\n",
      "borne 1\n",
      "besides 1\n",
      "sight 1\n",
      "thought 5\n",
      "object 3\n",
      "hatred 2\n",
      "constant 2\n",
      "either 4\n",
      "eurasia 3\n",
      "eastasia 2\n",
      "since 1\n",
      "oceania 3\n",
      "war 6\n",
      "one 8\n",
      "generally 1\n",
      "peace 1\n",
      "he 5\n",
      "sitting 1\n",
      "straight 1\n",
      "chair 1\n",
      "chest 1\n",
      "swelling 1\n",
      "quivering 1\n",
      "though 1\n",
      "standing 1\n",
      "assault 1\n",
      "wave 2\n",
      "helplessness 1\n",
      "doubt 2\n",
      "hung 1\n",
      "existence 2\n",
      "like 1\n",
      "sinister 1\n",
      "enchanter 1\n",
      "capable 1\n",
      "mere 3\n",
      "voice 2\n",
      "wrecking 1\n",
      "structure 2\n",
      "civilization 1\n",
      "sigh 1\n",
      "relief 1\n",
      "everybody 1\n",
      "hostile 1\n",
      "figure 1\n",
      "melted 1\n",
      "big 1\n",
      "brother 1\n",
      "full 2\n",
      "mysterious 1\n",
      "calm 2\n",
      "vast 1\n",
      "almost 3\n",
      "filled 1\n",
      "an 1\n",
      "smell 3\n",
      "sweat 2\n",
      "sort 1\n",
      "unconscious 1\n",
      "testimony 1\n",
      "strenuousness 1\n",
      "life 3\n",
      "followed 1\n",
      "wherever 1\n",
      "in 3\n",
      "public 1\n",
      "private 1\n",
      "utterance 1\n",
      "ever 4\n",
      "admitted 1\n",
      "three 10\n",
      "time 7\n",
      "grouped 1\n",
      "along 1\n",
      "different 1\n",
      "lines 1\n",
      "his 3\n",
      "sweating 1\n",
      "extraordinary 1\n",
      "moment 7\n",
      "mixed 1\n",
      "katharine 1\n",
      "white 1\n",
      "body 2\n",
      "frozen 1\n",
      "hypnotic 1\n",
      "party 11\n",
      "and 5\n",
      "yet 3\n",
      "frightening 1\n",
      "sounded 1\n",
      "cry 1\n",
      "hundred 1\n",
      "throats 1\n",
      "heart 1\n",
      "sank 1\n",
      "enormous 3\n",
      "arrayed 1\n",
      "ease 1\n",
      "intellectual 2\n",
      "would 8\n",
      "overthrow 1\n",
      "debate 1\n",
      "subtle 1\n",
      "arguments 2\n",
      "people 3\n",
      "thousands 1\n",
      "capitalists 1\n",
      "called 1\n",
      "rich 1\n",
      "bloke 1\n",
      "well 2\n",
      "i 5\n",
      "give 1\n",
      "name 1\n",
      "real 4\n",
      "speaker 1\n",
      "e 1\n",
      "might 1\n",
      "silenced 1\n",
      "girl 1\n",
      "acted 1\n",
      "quickly 1\n",
      "enough 2\n",
      "precisely 1\n",
      "extremity 1\n",
      "danger 3\n",
      "act 1\n",
      "first 4\n",
      "whiff 1\n",
      "scent 1\n",
      "stirred 1\n",
      "memory 1\n",
      "could 7\n",
      "pin 1\n",
      "troubling 1\n",
      "air 3\n",
      "little 1\n",
      "square 1\n",
      "chamber 1\n",
      "bells 1\n",
      "hot 1\n",
      "stagnant 1\n",
      "smelt 1\n",
      "pigeon 1\n",
      "dung 1\n",
      "she 1\n",
      "enjoyed 1\n",
      "work 1\n",
      "consisted 1\n",
      "chiefly 1\n",
      "running 1\n",
      "servicing 1\n",
      "tricky 1\n",
      "electric 1\n",
      "motor 1\n",
      "lunatic 1\n",
      "credulity 1\n",
      "needed 2\n",
      "members 1\n",
      "kept 1\n",
      "right 1\n",
      "pitch 1\n",
      "except 1\n",
      "bottling 1\n",
      "instinct 2\n",
      "using 1\n",
      "driving 1\n",
      "force 1\n",
      "whenever 1\n",
      "mouth 1\n",
      "corked 1\n",
      "clothes 1\n",
      "pegs 1\n",
      "singing 1\n",
      "contralto 1\n",
      "fancy 1\n",
      "rose 1\n",
      "saucepan 1\n",
      "exciting 1\n",
      "shut 1\n",
      "window 1\n",
      "lest 1\n",
      "anybody 1\n",
      "outside 1\n",
      "notice 1\n",
      "become 3\n",
      "inquisitive 1\n",
      "done 1\n",
      "persuade 1\n",
      "impulses 1\n",
      "feelings 1\n",
      "account 1\n",
      "robbing 1\n",
      "material 1\n",
      "world 5\n",
      "we 3\n",
      "shall 1\n",
      "utterly 1\n",
      "without 4\n",
      "kind 1\n",
      "betray 1\n",
      "country 1\n",
      "foreign 2\n",
      "encourage 1\n",
      "prostitution 1\n",
      "disseminate 1\n",
      "venereal 1\n",
      "diseases 1\n",
      "anything 2\n",
      "likely 2\n",
      "cause 1\n",
      "demoralization 1\n",
      "weaken 1\n",
      "for 4\n",
      "deprived 1\n",
      "speech 1\n",
      "when 1\n",
      "looked 2\n",
      "shoulders 1\n",
      "ugly 1\n",
      "civilized 1\n",
      "impossible 1\n",
      "believe 2\n",
      "defeated 1\n",
      "grip 1\n",
      "crushed 1\n",
      "bones 1\n",
      "winston 4\n",
      "palm 1\n",
      "absorption 1\n",
      "europe 1\n",
      "russia 1\n",
      "british 1\n",
      "empire 1\n",
      "united 1\n",
      "states 1\n",
      "two 2\n",
      "existing 1\n",
      "already 3\n",
      "effectively 1\n",
      "far 5\n",
      "direct 1\n",
      "economic 1\n",
      "purpose 1\n",
      "labour 3\n",
      "possession 1\n",
      "regions 1\n",
      "northern 1\n",
      "constantly 3\n",
      "struggling 2\n",
      "practice 1\n",
      "controls 2\n",
      "whole 1\n",
      "disputed 1\n",
      "area 1\n",
      "whichever 1\n",
      "equatorial 1\n",
      "africa 1\n",
      "countries 2\n",
      "middle 2\n",
      "east 1\n",
      "southern 1\n",
      "india 1\n",
      "indonesian 1\n",
      "coal 1\n",
      "oil 1\n",
      "race 1\n",
      "turn 2\n",
      "armaments 2\n",
      "capture 2\n",
      "territory 4\n",
      "control 1\n",
      "indefinitely 2\n",
      "mongolia 1\n",
      "dividing 1\n",
      "line 2\n",
      "never 2\n",
      "stable 1\n",
      "round 2\n",
      "pole 1\n",
      "lay 3\n",
      "claim 2\n",
      "territories 2\n",
      "fact 4\n",
      "largely 2\n",
      "uninhabited 2\n",
      "unexplored 2\n",
      "balance 2\n",
      "powers 2\n",
      "always 7\n",
      "remains 3\n",
      "roughly 1\n",
      "forms 1\n",
      "heartland 1\n",
      "imagine 1\n",
      "society 2\n",
      "wealth 3\n",
      "sense 1\n",
      "personal 1\n",
      "possessions 1\n",
      "luxuries 1\n",
      "evenly 1\n",
      "distributed 1\n",
      "remained 1\n",
      "hands 1\n",
      "small 2\n",
      "privileged 1\n",
      "caste 2\n",
      "weapons 1\n",
      "actually 1\n",
      "destroyed 1\n",
      "manufacture 1\n",
      "still 1\n",
      "convenient 1\n",
      "way 2\n",
      "expending 1\n",
      "producing 1\n",
      "consumed 1\n",
      "consciousness 1\n",
      "therefore 3\n",
      "makes 1\n",
      "seem 2\n",
      "natural 1\n",
      "unavoidable 1\n",
      "condition 1\n",
      "survival 1\n",
      "achieved 1\n",
      "gradually 1\n",
      "acquiring 1\n",
      "building 1\n",
      "overwhelming 1\n",
      "preponderance 1\n",
      "discovery 1\n",
      "new 4\n",
      "unanswerable 1\n",
      "weapon 3\n",
      "planning 1\n",
      "logistics 1\n",
      "future 1\n",
      "wars 1\n",
      "others 3\n",
      "devise 1\n",
      "larger 2\n",
      "rocket 1\n",
      "bombs 3\n",
      "explosives 1\n",
      "impenetrable 1\n",
      "search 1\n",
      "deadlier 1\n",
      "gases 1\n",
      "soluble 1\n",
      "poisons 1\n",
      "what 3\n",
      "remarkable 2\n",
      "possess 2\n",
      "atomic 4\n",
      "bomb 2\n",
      "powerful 1\n",
      "present 3\n",
      "researches 2\n",
      "discover 1\n",
      "groups 3\n",
      "mean 1\n",
      "end 2\n",
      "organized 1\n",
      "hence 1\n",
      "all 2\n",
      "continue 1\n",
      "produce 1\n",
      "store 1\n",
      "decisive 1\n",
      "opportunity 1\n",
      "strategy 1\n",
      "following 2\n",
      "pretend 1\n",
      "usual 1\n",
      "ruling 3\n",
      "simultaneously 2\n",
      "aware 1\n",
      "unaware 1\n",
      "product 1\n",
      "mind 2\n",
      "similar 1\n",
      "enormously 1\n",
      "systematic 1\n",
      "less 4\n",
      "long 4\n",
      "periods 1\n",
      "high 1\n",
      "securely 1\n",
      "sooner 1\n",
      "later 1\n",
      "comes 1\n",
      "lose 1\n",
      "belief 2\n",
      "capacity 1\n",
      "made 1\n",
      "use 1\n",
      "terms 1\n",
      "freedom 1\n",
      "justice 1\n",
      "fraternity 1\n",
      "point 2\n",
      "view 1\n",
      "seizing 1\n",
      "human 5\n",
      "equality 1\n",
      "longer 1\n",
      "ideal 1\n",
      "striven 1\n",
      "averted 1\n",
      "numbers 1\n",
      "past 4\n",
      "ages 1\n",
      "avaricious 1\n",
      "tempted 1\n",
      "luxury 3\n",
      "hungrier 1\n",
      "pure 4\n",
      "conscious 1\n",
      "intent 1\n",
      "crushing 1\n",
      "opposition 2\n",
      "part 1\n",
      "reason 1\n",
      "government 2\n",
      "keep 1\n",
      "citizens 1\n",
      "surveillance 1\n",
      "there 2\n",
      "four 1\n",
      "ways 1\n",
      "group 1\n",
      "fall 1\n",
      "a 1\n",
      "class 1\n",
      "guard 1\n",
      "remain 1\n",
      "permanently 1\n",
      "each 1\n",
      "divide 1\n",
      "unconquerable 2\n",
      "conquerable 2\n",
      "slow 2\n",
      "demographic 2\n",
      "changes 1\n",
      "wide 1\n",
      "easily 1\n",
      "avert 1\n",
      "aim 2\n",
      "transmitting 1\n",
      "children 1\n",
      "keeping 1\n",
      "ablest 1\n",
      "top 1\n",
      "perfectly 1\n",
      "prepared 1\n",
      "who 1\n",
      "wields 1\n",
      "important 2\n",
      "provided 1\n",
      "hierarchical 1\n",
      "generation 1\n",
      "century 2\n",
      "working 1\n",
      "breeding 1\n",
      "dying 1\n",
      "impulse 1\n",
      "rebel 1\n",
      "grasping 2\n",
      "live 1\n",
      "continuous 2\n",
      "frenzy 1\n",
      "enemies 1\n",
      "internal 1\n",
      "traitors 1\n",
      "triumph 4\n",
      "victories 1\n",
      "wisdom 1\n",
      "includes 1\n",
      "analogies 1\n",
      "failing 1\n",
      "perceive 1\n",
      "logical 1\n",
      "errors 2\n",
      "misunderstanding 1\n",
      "simplest 1\n",
      "doublethink 3\n",
      "means 4\n",
      "holding 1\n",
      "contradictory 1\n",
      "beliefs 1\n",
      "accepting 1\n",
      "oligarchies 1\n",
      "fallen 1\n",
      "ossified 1\n",
      "grew 1\n",
      "soft 1\n",
      "secret 2\n",
      "rulership 1\n",
      "combine 1\n",
      "infallibility 1\n",
      "learn 1\n",
      "mistakes 1\n",
      "reconciling 1\n",
      "contradictions 1\n",
      "retained 1\n",
      "but 3\n",
      "deeper 1\n",
      "lies 1\n",
      "original 1\n",
      "motive 1\n",
      "led 1\n",
      "seizure 1\n",
      "brought 2\n",
      "police 1\n",
      "warfare 1\n",
      "necessary 1\n",
      "paraphernalia 1\n",
      "as 2\n",
      "woman 1\n",
      "characteristic 1\n",
      "attitude 1\n",
      "thick 1\n",
      "arms 1\n",
      "reaching 1\n",
      "buttocks 1\n",
      "protruded 1\n",
      "struck 1\n",
      "beautiful 1\n",
      "exactly 1\n",
      "learned 1\n",
      "think 1\n",
      "storing 1\n",
      "hearts 1\n",
      "bellies 1\n",
      "muscles 1\n",
      "day 4\n",
      "overturn 1\n",
      "door 1\n",
      "opened 1\n",
      "created 1\n",
      "cold 1\n",
      "glaring 1\n",
      "lights 1\n",
      "eyes 1\n",
      "ran 1\n",
      "water 1\n",
      "simply 2\n",
      "humiliate 1\n",
      "destroy 1\n",
      "arguing 1\n",
      "reasoning 2\n",
      "will 1\n",
      "please 1\n",
      "remember 1\n",
      "throughout 1\n",
      "conversation 1\n",
      "inflict 1\n",
      "pain 3\n",
      "whatever 1\n",
      "degree 1\n",
      "choose 1\n",
      "at 1\n",
      "intolerable 1\n",
      "us 1\n",
      "erroneous 1\n",
      "exist 1\n",
      "anywhere 1\n",
      "however 1\n",
      "may 1\n",
      "you 2\n",
      "understand 2\n",
      "how 1\n",
      "maintains 1\n",
      "now 1\n",
      "tell 1\n",
      "why 2\n",
      "cling 1\n",
      "want 1\n",
      "that 2\n",
      "seek 1\n",
      "ends 1\n",
      "good 2\n",
      "majority 1\n",
      "sought 1\n",
      "men 5\n",
      "mass 1\n",
      "frail 1\n",
      "cowardly 1\n",
      "creatures 1\n",
      "endure 1\n",
      "liberty 1\n",
      "seeks 1\n",
      "entirely 1\n",
      "sake 1\n",
      "interested 2\n",
      "solely 1\n",
      "not 2\n",
      "happiness 2\n",
      "presently 1\n",
      "they 1\n",
      "pretended 1\n",
      "perhaps 1\n",
      "believed 1\n",
      "seized 2\n",
      "unwillingly 1\n",
      "limited 1\n",
      "corner 1\n",
      "paradise 1\n",
      "beings 3\n",
      "know 1\n",
      "seizes 1\n",
      "intention 1\n",
      "relinquishing 1\n",
      "thinking 1\n",
      "talk 1\n",
      "able 1\n",
      "prevent 1\n",
      "decay 1\n",
      "priests 1\n",
      "said 1\n",
      "word 1\n",
      "concerned 1\n",
      "gather 1\n",
      "idea 1\n",
      "thing 3\n",
      "must 1\n",
      "realize 3\n",
      "collective 1\n",
      "individual 2\n",
      "ceases 1\n",
      "second 2\n",
      "matter 1\n",
      "external 1\n",
      "reality 1\n",
      "call 1\n",
      "fight 3\n",
      "night 3\n",
      "things 3\n",
      "assumed 1\n",
      "schoolmaster 1\n",
      "questioning 1\n",
      "promising 1\n",
      "pupil 1\n",
      "man 1\n",
      "assert 1\n",
      "another 1\n",
      "inflicting 1\n",
      "humiliation 1\n",
      "tearing 1\n",
      "minds 1\n",
      "pieces 1\n",
      "putting 1\n",
      "together 1\n",
      "shapes 1\n",
      "choosing 1\n",
      "forget 1\n",
      "intoxication 1\n",
      "increasing 1\n",
      "growing 1\n",
      "subtler 1\n",
      "tolerant 1\n",
      "weaker 1\n",
      "tighter 1\n",
      "despotism 1\n",
      "victory 1\n",
      "endless 1\n",
      "pressing 3\n",
      "upon 1\n",
      "nerve 1\n",
      "remaining 1\n",
      "front 1\n",
      "teeth 1\n",
      "thumb 1\n",
      "forefinger 1\n",
      "effort 1\n",
      "stimulus 1\n",
      "removed 1\n",
      "helpless 1\n",
      "iron 1\n",
      "telescreen 1\n",
      "told 1\n",
      "grasped 1\n",
      "frivolity 1\n",
      "shallowness 1\n",
      "attempt 1\n",
      "set 1\n",
      "wrote 1\n",
      "god 1\n",
      "is 1\n",
      "accepted 1\n",
      "everything 1\n",
      "great 1\n",
      "improvisation 1\n",
      "huge 1\n",
      "gazed 1\n",
      "back 1\n",
      "many 1\n",
      "crimes 1\n",
      "beyond 1\n",
      "commit 1\n",
      "nameless 1\n",
      "unimaginable 1\n",
      "secure 1\n",
      "these 1\n",
      "rights 1\n",
      "governments 1\n",
      "are 1\n",
      "instituted 1\n",
      "among 1\n",
      "deriving 1\n",
      "their 1\n",
      "from 1\n",
      "consent 1\n",
      "of 1\n",
      "governed 1\n"
     ]
    }
   ],
   "source": [
    "freq = collocation( full_text , r'power' , 20)\n",
    "\n",
    "for nearby_word in freq:\n",
    "    print(nearby_word , freq[nearby_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7adb77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjective_codes = ['JJ','JJR','JJS']\n",
    "adverb_codes = ['RB','RBR','RBS']\n",
    "\n",
    "adjectives = Counter()\n",
    "adverbs = Counter()\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words = remove_punctuation(words)\n",
    "    \n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    for p in pos:\n",
    "    \n",
    "        if p[1] in adjective_codes:\n",
    "            adjectives.update([p[0]])\n",
    "\n",
    "        if p[1] in adverb_codes:\n",
    "            adverbs.update([p[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194da397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('other', 168)\n",
      "('same', 118)\n",
      "('little', 109)\n",
      "('own', 105)\n",
      "('old', 103)\n",
      "('few', 98)\n",
      "('more', 95)\n",
      "('possible', 73)\n",
      "('such', 73)\n",
      "('human', 69)\n"
     ]
    }
   ],
   "source": [
    "for word in adjectives.most_common(10):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233efb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('not', 833)\n",
      "('even', 246)\n",
      "('only', 190)\n",
      "('so', 173)\n",
      "('again', 169)\n",
      "('never', 166)\n",
      "('still', 147)\n",
      "('then', 146)\n",
      "('always', 136)\n",
      "('very', 121)\n"
     ]
    }
   ],
   "source": [
    "for word in adverbs.most_common(10):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79849b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ana = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa87813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words:\n",
      "party\n",
      "like\n",
      "kind\n",
      "hand\n",
      "feeling\n",
      "well\n",
      "certain\n",
      "good\n",
      "great\n",
      "true\n",
      "better\n",
      "love\n",
      "'yes\n",
      "heart\n",
      "matter\n",
      "truth\n",
      "sure\n",
      "want\n",
      "top\n",
      "important\n",
      "\n",
      "Negative words:\n",
      "no\n",
      "war\n",
      "pain\n",
      "death\n",
      "stopped\n",
      "alone\n",
      "hate\n",
      "dead\n",
      "fear\n",
      "enemy\n",
      "prisoners\n",
      "difficult\n",
      "hatred\n",
      "forgotten\n",
      "lies\n",
      "struck\n",
      "stop\n",
      "broken\n",
      "lost\n",
      "dangerous\n"
     ]
    }
   ],
   "source": [
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    for word in words:\n",
    "        scores = ana.polarity_scores(word)\n",
    "        if scores[\"pos\"] > 0.75:\n",
    "            positive_words.append(word)\n",
    "        elif scores[\"neg\"] > 0.75:\n",
    "            negative_words.append(word)\n",
    "        \n",
    "print('Positive words:')\n",
    "\n",
    "sentiment_freq = Counter(positive_words)\n",
    "\n",
    "for word,count in sentiment_freq.most_common(20):\n",
    "    print(word)\n",
    "    \n",
    "    \n",
    "print('\\nNegative words:')\n",
    "\n",
    "sentiment_freq = Counter(negative_words)\n",
    "\n",
    "for word,count in sentiment_freq.most_common(20):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c6e6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Postive sentences\n",
      "\n",
      "Never again will you be capable of love, or friendship, or joy of living, or laughter, or curiosity, or courage, or integrity. [0.9702]\n",
      "He was abusing Big Brother, he was denouncing the dictatorship of the Party, he was demanding the immediate conclusion of peace with Eurasia, he was advocating freedom of speech, freedom of the Press, freedom of assembly, freedom of thought, he was crying hysterically that the revolution had been betrayed--and all this in rapid polysyllabic speech which was a sort of parody of the habitual style of the orators of the Party, and even contained Newspeak words: more Newspeak words, indeed, than any Party member would normally use in real life. [0.9698]\n",
      "That the choice for mankind lay between freedom and happiness, and that, for the great bulk of mankind, happiness was better. [0.9607]\n",
      "Take for example the well-known passage from the Declaration of Independence: WE HOLD THESE TRUTHS TO BE SELF-EVIDENT, THAT ALL MEN ARE CREATED EQUAL, THAT THEY ARE ENDOWED BY THEIR CREATOR WITH CERTAIN INALIENABLE RIGHTS, THAT AMONG THESE ARE LIFE, LIBERTY, AND THE PURSUIT OF HAPPINESS. [0.9556]\n",
      "Day and night the telescreens bruised your ears with statistics proving that people today had more food, more clothes, better houses, better recreations--that they lived longer, worked shorter hours, were bigger, healthier, stronger, happier, more intelligent, better educated, than the people of fifty years ago. [0.9555]\n",
      "Every success, every achievement, every victory, every scientific discovery, all knowledge, all wisdom, all happiness, all virtue, are held to issue directly from his leadership and inspiration. [0.9509]\n",
      "Nevertheless, the few luxuries that he does enjoy his large, well-appointed flat, the better texture of his clothes, the better quality of his food and drink and tobacco, his two or three servants, his private motor-car or helicopter--set him in a different world from a member of the Outer Party, and the members of the Outer Party have a similar advantage in comparison with the submerged masses whom we call 'the proles'. [0.9371]\n",
      "It could not be used in its old sense of 'politically free' or 'intellectually free' since political and intellectual freedom no longer existed even as concepts, and were therefore of necessity nameless. [0.936]\n",
      "They lived in great gorgeous houses with thirty servants, they rode about in motor-cars and four-horse carriages, they drank champagne, they wore top hats----' The old man brightened suddenly. [0.9349]\n",
      "But from your general appearance--merely because you're young and fresh and healthy, you understand--I thought that probably----' 'You thought I was a good Party member. [0.9313]\n",
      "\n",
      "Negative sentences\n",
      "\n",
      "But what was strange was that although Goldstein was hated and despised by everybody, although every day and a thousand times a day, on platforms, on the telescreen, in newspapers, in books, his theories were refuted, smashed, ridiculed, held up to the general gaze for the pitiful rubbish that they were--in spite of all this, his influence never seemed to grow less. [-0.9769]\n",
      "On the battlefield, in the torture chamber, on a sinking ship, the issues that you are fighting for are always forgotten, because the body swells up until it fills the universe, and even when you are not paralysed by fright or screaming with pain, life is a moment-to-moment struggle against hunger or cold or sleeplessness, against a sour stomach or an aching tooth. [-0.971]\n",
      "His voice, made metallic by the amplifiers, boomed forth an endless catalogue of atrocities, massacres, deportations, lootings, rapings, torture of prisoners, bombing of civilians, lying propaganda, unjust aggressions, broken treaties. [-0.9652]\n",
      "Because the Inquisition killed its enemies in the open, and killed them while they were still unrepentant: in fact, it killed them because they were unrepentant. [-0.9633]\n",
      "And in spite of the endless slaughters reported in the Press and on the telescreens, the desperate battles of earlier wars, in which hundreds of thousands or even millions of men were often killed in a few weeks, have never been repeated. [-0.9468]\n",
      "They slapped his face, wrung his ears, pulled his hair, made him stand on one leg, refused him leave to urinate, shone glaring lights in his face until his eyes ran with water; but the aim of this was simply to humiliate him and destroy his power of arguing and reasoning. [-0.9451]\n",
      "In front of him was an enemy who was trying to kill him: in front of him, also, was a human creature, in pain and perhaps with a broken bone. [-0.9393]\n",
      "But before death (nobody spoke of such things, yet everybody knew of them) there was the routine of confession that had to be gone through: the grovelling on the floor and screaming for mercy, the crack of broken bones, the smashed teeth and bloody clots of hair. [-0.9382]\n",
      "Already, at the time when he made his discovery, Oceania was no longer at war with Eurasia, and it must have been to the agents of Eastasia that the three dead men had betrayed their country. [-0.9371]\n",
      "A low-ceilinged, crowded room, its walls grimy from the contact of innumerable bodies; battered metal tables and chairs, placed so close together that you sat with elbows touching; bent spoons, dented trays, coarse white mugs; all surfaces greasy, grime in every crack; and a sourish, composite smell of bad gin and bad coffee and metallic stew and dirty clothes. [-0.9349]\n"
     ]
    }
   ],
   "source": [
    "sent_scores = dict()\n",
    "\n",
    "sentences = [re.sub('\\n+', ' ' , sent) for sent in sentences]\n",
    "\n",
    "for s in sentences:\n",
    "    scores = ana.polarity_scores(s)\n",
    "    sent_scores[s] = scores['compound']\n",
    "        \n",
    "nr_sentences = 10\n",
    "\n",
    "i = 0\n",
    "\n",
    "print('\\nPostive sentences\\n')\n",
    "\n",
    "for s in sorted_by_value( sent_scores , ascending = False ):\n",
    "    print( f'{s} [{ sent_scores[s]}]' )\n",
    "    i+= 1\n",
    "    if i == nr_sentences:\n",
    "        break\n",
    "        \n",
    "print('\\nNegative sentences\\n')\n",
    "i = 0\n",
    "        \n",
    "for s in sorted_by_value( sent_scores , ascending = True):\n",
    "    print( f'{s} [{ sent_scores[s]}]' )\n",
    "    i+= 1\n",
    "    if i == nr_sentences:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d5a9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def wordnet_hypernyms(token):\n",
    "    all_hypernyms = []\n",
    "    black_list = ['queen','young','human']\n",
    "    \n",
    "    if token not in black_list:\n",
    "        \n",
    "        word_senses = wn.synsets(token)\n",
    "        hypernyms = lambda s: s.hypernyms()\n",
    "        return_value = False\n",
    "        for ws in word_senses:\n",
    "\n",
    "            hypernyms = [hyp.name() for hyp in list(ws.closure(hypernyms))]\n",
    "            if 'plant.n.02' in hypernyms:\n",
    "                all_hypernyms.append('plant')\n",
    "            if 'color.n.01' in hypernyms:\n",
    "                all_hypernyms.append('colour')\n",
    "            if 'emotion.n.01' in hypernyms:\n",
    "                all_hypernyms.append('emotion')\n",
    "            if 'animal.n.01' in hypernyms:\n",
    "                all_hypernyms.append('animal')\n",
    "            if 'natural_phenomenon.n.01' in hypernyms:\n",
    "                all_hypernyms.append('natural_phenomenon')\n",
    "            if 'body_part.n.01' in hypernyms:\n",
    "                all_hypernyms.append('body_part')\n",
    "                                    \n",
    "    return all_hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "029ba9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common emotions:\n",
      "love\n",
      "hate\n",
      "fear\n",
      "hatred\n",
      "spite\n",
      "horror\n",
      "panic\n",
      "terror\n",
      "fit\n",
      "happiness\n",
      "fright\n",
      "rage\n",
      "reverence\n",
      "uneasiness\n",
      "exaltation\n",
      "\n",
      "Most common colours:\n",
      "black\n",
      "blue\n",
      "red\n",
      "yellow\n",
      "scarlet\n",
      "brown\n",
      "pink\n",
      "green\n",
      "blackness\n",
      "coral\n",
      "sanguine\n",
      "whiteness\n",
      "magenta\n",
      "purple\n",
      "\n",
      "Most common body parts:\n",
      "face\n",
      "back\n",
      "hand\n",
      "head\n",
      "small\n",
      "behind\n",
      "fingers\n",
      "arm\n",
      "mouth\n",
      "faces\n",
      "feet\n",
      "nose\n",
      "skin\n",
      "knees\n",
      "shoulders\n",
      "\n",
      "Most common natural phenomena:\n",
      "light\n",
      "low\n",
      "wind\n",
      "current\n",
      "quiet\n",
      "lights\n",
      "thrust\n",
      "sunlight\n",
      "pressure\n",
      "smoke\n",
      "chop\n",
      "heat\n",
      "breeze\n",
      "lamplight\n",
      "gravity\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "\n",
    "for word,count in word_frequencies.most_common():\n",
    "    hypernyms = wordnet_hypernyms(word)\n",
    "    if len(hypernyms)>0:\n",
    "        for h in hypernyms:\n",
    "            word_list.append((word,h))\n",
    "\n",
    "def find_most_common_hyponyms(domain):\n",
    "    common_hyponyms = []\n",
    "    for word, hyponym in word_list:\n",
    "        if hyponym == domain:\n",
    "            common_hyponyms.append(word)\n",
    "    return Counter(common_hyponyms)\n",
    "        \n",
    "print(f\"Most common emotions:\")\n",
    "common_hyponyms = find_most_common_hyponyms('emotion')\n",
    "for word,count in common_hyponyms.most_common(15):\n",
    "    print(word)\n",
    "    \n",
    "print(f\"\\nMost common colours:\")\n",
    "common_hyponyms = find_most_common_hyponyms('colour')\n",
    "for word,count in common_hyponyms.most_common(15):\n",
    "    print(word)\n",
    "    \n",
    "print(f\"\\nMost common body parts:\")\n",
    "common_hyponyms = find_most_common_hyponyms('body_part')\n",
    "for word,count in common_hyponyms.most_common(15):\n",
    "    print(word)\n",
    "    \n",
    "common_hyponyms = find_most_common_hyponyms('natural_phenomenon')\n",
    "if len(common_hyponyms)>0:\n",
    "    print(f\"\\nMost common natural phenomena:\")\n",
    "    for word,count in common_hyponyms.most_common(15):\n",
    "        print(word)\n",
    "\n",
    "    \n",
    "common_hyponyms = find_most_common_hyponyms('amimal')\n",
    "if len(common_hyponyms)>0:\n",
    "    print(f\"\\nMost common animals:\")\n",
    "    for word,count in common_hyponyms.most_common(15):\n",
    "        print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
